{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ソーシャルディスタンスを検知するアプリを作ろう\n",
    "インテル® OpenVINO™ ツールキットの事前学習済みモデルを使ってソーシャルディスタンスを検知するアプリケーションを作成します。事前学習済みモデルを利用することで簡単にAI機能を含んだアプリケーションを開発できることをご体験ください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第1章 人物検出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP0. 事前学習済みモデルのダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 $INTEL_OPENVINO_DIR/deployment_tools/tools/model_downloader/downloader.py --name person-detection-retail-0013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP1. モジュールのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import os\n",
    "import io\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "\n",
    "import logging as log\n",
    "from PIL import Image\n",
    "import PIL\n",
    "\n",
    "from munkres import Munkres\n",
    "\n",
    "from openvino.inference_engine import IENetwork, IECore\n",
    "\n",
    "import IPython.display\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP2. 必要なクラスを定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#検出したPersonの各種情報を保持しておくためのデータホルダークラス\n",
    "class data_base_h:\n",
    "    def __init__(self, position, feature=[], id=-1):\n",
    "        self.pos = position\n",
    "        self.feature = feature\n",
    "        self.time = time.monotonic()\n",
    "        self.id = id\n",
    "\n",
    "#Personを検出するためのディープラーニング推論をOpenVINOで実行するためのクラス\n",
    "class PersonDetector:\n",
    "    \n",
    "    #検出した物体がPersonであるか否かを判定するための閾値\n",
    "    THRESHOLD = 0.3\n",
    "    \n",
    "    def __init__(self, iecore, model_path):\n",
    "        #AI model of human recognition　settings\n",
    "        self.net_h  = net = iecore.read_network(model = model_path+ \".xml\", weights = model_path + \".bin\") \n",
    "        self.input_name_h  = next(iter(self.net_h.inputs))                     \n",
    "        self.input_shape_h = self.net_h.inputs[self.input_name_h].shape           \n",
    "        self.out_name_h    = next(iter(self.net_h.outputs))                    \n",
    "        self.out_shape_h   = self.net_h.outputs[self.out_name_h].shape  \n",
    "        self.exec_net_h    = iecore.load_network(self.net_h, 'CPU')\n",
    "    \n",
    "    def inference_and_get_person_bbox(self, image):\n",
    "        #AI model \"human recognition　settings\" inference\n",
    "        in_frame = cv2.resize(image, (self.input_shape_h[3], self.input_shape_h[2]))\n",
    "        in_frame = in_frame.transpose((2, 0, 1))\n",
    "        in_frame = in_frame.reshape(self.input_shape_h)\n",
    "        res_h = self.exec_net_h.infer(inputs={self.input_name_h: in_frame})\n",
    "        person_bbox_list = res_h[self.out_name_h][0][0]\n",
    "        \n",
    "        detected_person_list = []\n",
    "        for person_bbox in person_bbox_list:\n",
    "            probability = person_bbox[2]\n",
    "            if probability > PersonDetector.THRESHOLD: \n",
    "                frame = image\n",
    "                xmin = abs(int(person_bbox[3] * frame.shape[1]))\n",
    "                ymin = abs(int(person_bbox[4] * frame.shape[0]))\n",
    "                xmax = abs(int(person_bbox[5] * frame.shape[1]))\n",
    "                ymax = abs(int(person_bbox[6] * frame.shape[0]))\n",
    "\n",
    "                person_image = frame[ymin:ymax, xmin:xmax]   \n",
    "                detected_person_list.append(data_base_h([xmin, ymin, xmax, ymax]))\n",
    "        return detected_person_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP3. 各種ユーティリティー関数を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#検出したPersonにモザイク処理をするための関数\n",
    "def mosaic_area(img, xmin, ymin, xmax, ymax, ratio=0.05):\n",
    "    dst = img.copy()\n",
    "    dst[ymin:ymax,xmin:xmax] = mosaic(dst[ymin:ymax,xmin:xmax], ratio)\n",
    "    return dst\n",
    "\n",
    "def mosaic(img, ratio=0.05):\n",
    "    small = cv2.resize(img, None, fx=ratio, fy=ratio, interpolation=cv2.INTER_NEAREST)\n",
    "    big = cv2.resize(small, img.shape[:2][::-1], interpolation=cv2.INTER_NEAREST)\n",
    "    return big\n",
    "\n",
    "\n",
    "#推論結果を反映した画面出力用の画像（フレーム）を作成する関数\n",
    "def create_output_image(image, detected_person_list, mosaic=False):\n",
    "    for detected_person in detected_person_list:\n",
    "        id = detected_person.id\n",
    "        color = (0, 0, 0)\n",
    "        xmin, ymin, xmax, ymax = detected_person.pos\n",
    "        if mosaic:\n",
    "            image = mosaic_area(image, xmin, ymin, xmax, ymax)\n",
    "        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), color, 5)\n",
    "    font = cv2.FONT_HERSHEY_COMPLEX\n",
    "    cv2.putText(image, 'PersonDetection', (50, 150), font, 4, (0, 0, 255), 2, cv2.LINE_AA)              \n",
    "    image = cv2.resize(image, dsize=(600, 360))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP4. メイン関数を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(video_file):\n",
    "\n",
    "    #Instantiate two AI models\n",
    "    iecore = IECore()\n",
    "    detector = PersonDetector(iecore, \"intel/person-detection-retail-0013/FP32/person-detection-retail-0013\")\n",
    "   \n",
    "    #Please enter the name of the video file you want to process\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    \n",
    "    start_time = time.monotonic()\n",
    "    \n",
    "    index = 0 #Index setting\n",
    "    while cv2.waitKey(1) != 27:\n",
    "        index += 1\n",
    "        \n",
    "        #Read one frame from the video data\n",
    "        frame_data = cap.read()\n",
    "        if frame_data[0] == False:\n",
    "            return\n",
    "        \n",
    "        #Set captured frame\n",
    "        image = frame_data[1]\n",
    "\n",
    "        #Detect all person's bounding box in the captured frame\n",
    "        detected_person_list = detector.inference_and_get_person_bbox(image)\n",
    "\n",
    "        #Window display processing           \n",
    "        image = create_output_image(image, detected_person_list)\n",
    "        #cv2.imshow('Frame', image)\n",
    "        clear_output(wait=True)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        f = io.BytesIO()\n",
    "        PIL.Image.fromarray(image).save(f, 'jpeg')\n",
    "        IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
    "        \n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP5. 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main('people.264')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第2章 人物認証"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP0. 事前学習済みモデルのダウンロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 $INTEL_OPENVINO_DIR/deployment_tools/tools/model_downloader/downloader.py --name person-reidentification-retail-0287"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP1. 人物認証処理を実行する用のクラスを定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonIdentifier:\n",
    "    def __init__(self, iecore, model_path):\n",
    "        #AI model of personal identification settings\n",
    "        self.net_p = iecore.read_network(model = model_path + \".xml\", weights = model_path + \".bin\") \n",
    "        self.input_name_p  = next(iter(self.net_p.inputs))                  \n",
    "        self.input_shape_p = self.net_p.inputs[self.input_name_p].shape        \n",
    "        self.out_name_p    = next(iter(self.net_p.outputs))                 \n",
    "        self.out_shape_p   = self.net_p.outputs[self.out_name_p].shape \n",
    "        self.exec_net_p    = iecore.load_network(self.net_p, 'CPU')\n",
    "        \n",
    "        #Person identification management\n",
    "        self.id_num = 0\n",
    "        self.dist_threshold = 1.0\n",
    "        self.timeout_threshold = 10000\n",
    "        self.feature_db = []\n",
    "    \n",
    "    def inference_and_get_feature(self, obj_img):\n",
    "        obj_img = cv2.resize(obj_img, (128, 256)) \n",
    "        obj_img = obj_img.transpose((2,0,1))\n",
    "        obj_img = np.expand_dims(obj_img, axis=0)\n",
    "\n",
    "        #AI model \"Personal recognition\" inference             \n",
    "        res_reid = self.exec_net_p.infer(inputs={ self.input_name_p : obj_img}) \n",
    "        feature = np.array(res_reid[self.out_name_p]).reshape((256))\n",
    "        return feature\n",
    "    \n",
    "    def register_into_database(self, detected_person_list):\n",
    "        hangarian = Munkres()\n",
    "        dist_matrix = [ [ distance.cosine(obj_db.feature, obj_cam.feature) for obj_db in self.feature_db ] for obj_cam in detected_person_list ]\n",
    "        combination = hangarian.compute(dist_matrix)      \n",
    "        for idx_obj, idx_db in combination:\n",
    "            if detected_person_list[idx_obj].id!=-1: \n",
    "                continue \n",
    "            dist = distance.cosine(detected_person_list[idx_obj].feature, self.feature_db[idx_db].feature)\n",
    "            if dist < self.dist_threshold:\n",
    "                self.feature_db[idx_db].time = time.monotonic()            \n",
    "                detected_person_list[idx_obj].id = self.feature_db[idx_db].id              \n",
    "        del hangarian\n",
    "        \n",
    "        for detected_person in detected_person_list:\n",
    "            if detected_person.id == -1:\n",
    "                xmin, ymin, xmax, ymax = detected_person.pos\n",
    "                detected_person.id = self.id_num\n",
    "                self.feature_db.append(detected_person)\n",
    "                self.id_num += 1\n",
    "        \n",
    "        for i, db in enumerate(self.feature_db):\n",
    "            if time.monotonic() - db.time > self.timeout_threshold:\n",
    "                self.feature_db.pop(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP2. 画面表示用の関数を少し修正"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_output_image_with_id(image, detected_person_list, mosaic=False):\n",
    "    for detected_person in detected_person_list:\n",
    "        id = detected_person.id\n",
    "        color = (0, 0, 0)\n",
    "        xmin, ymin, xmax, ymax = detected_person.pos\n",
    "        if mosaic:\n",
    "            image = mosaic_area(image, xmin, ymin, xmax, ymax)\n",
    "        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), color, 5)\n",
    "        cv2.putText(image, str(id), (xmin, ymin - 7), cv2.FONT_HERSHEY_COMPLEX, 1.0, color, 1)\n",
    "    font = cv2.FONT_HERSHEY_COMPLEX\n",
    "    cv2.putText(image, 'PersonDetection', (50, 150), font, 4, (0, 0, 255), 2, cv2.LINE_AA)              \n",
    "    image = cv2.resize(image, dsize=(600, 360))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP3. メイン関数を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(video_file):\n",
    "    #Instantiate two AI models\n",
    "    iecore = IECore()\n",
    "    detector = PersonDetector(iecore, \"intel/person-detection-retail-0013/FP32/person-detection-retail-0013\")\n",
    "    identifier = PersonIdentifier(iecore, \"intel/person-reidentification-retail-0287/FP32/person-reidentification-retail-0287\")\n",
    "       \n",
    "    #Please enter the name of the video file you want to process\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    \n",
    "    start_time = time.monotonic()\n",
    "    \n",
    "    index = 0 #Index setting\n",
    "    while cv2.waitKey(1) != 27:    \n",
    "        index += 1\n",
    "        \n",
    "        frame = cap.read()\n",
    "        if frame[0] == False:\n",
    "            return\n",
    "        \n",
    "        image = frame[1]\n",
    "\n",
    "        #AI model \"human recognition　settings\" inference\n",
    "        detected_person_list = detector.inference_and_get_person_bbox(image)\n",
    "\n",
    "        for detected_person in detected_person_list:\n",
    "            xmin, ymin, xmax, ymax = detected_person.pos\n",
    "            person_image = image[ymin:ymax, xmin:xmax] \n",
    "            feature = identifier.inference_and_get_feature(person_image)    \n",
    "            detected_person.feature = feature\n",
    "\n",
    "        #Register vectors into the database\n",
    "        identifier.register_into_database(detected_person_list)\n",
    "        \n",
    "        #Window display processing           \n",
    "        image = create_output_image_with_id(image, detected_person_list)\n",
    "        #cv2.imshow('Frame', image)\n",
    "        clear_output(wait=True)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        f = io.BytesIO()\n",
    "        PIL.Image.fromarray(image).save(f, 'jpeg')\n",
    "        IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
    "        \n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP4. 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main('people.264')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第3章 ソーシャルディスタンス検知"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP1. ソーシャルディスタンス検知処理用のクラスを定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SocialDistanceViolationJudge:\n",
    "    '''\n",
    "    Please custom Social_parameter\n",
    "    SOCIAL_PARAMETER determines Social_Distance.\n",
    "    But that's not the actual distance. This is the distance between the x and y coordinates on the screen.\n",
    "    '''\n",
    "    SOCIAL_PARAMETER = 50.0\n",
    "\n",
    "    def __init__(self):\n",
    "        self.violate_t = list()\n",
    "    \n",
    "   \n",
    "    def judge_violation_of_social_distance(self, object_H, index, start_time):\n",
    "        violate = set() \n",
    "        violate_b = set()\n",
    "        centroids = np.array([[(obj.pos[2]-obj.pos[0])/2+obj.pos[0],(obj.pos[3]-obj.pos[1])/2+obj.pos[1]] for obj in object_H])\n",
    "        D_1 = distance.cdist(centroids, centroids, metric = \"euclidean\")        \n",
    "        for i in range(0, D_1.shape[0]):\n",
    "            for j in range(i+1, D_1.shape[1]):\n",
    "                if  (D_1[i,j] != 0.0 and D_1[i, j] < SocialDistanceViolationJudge.SOCIAL_PARAMETER):\n",
    "                    violate.add(object_H[i].id)\n",
    "                    violate.add(object_H[j].id)                 \n",
    "        if index == 1:\n",
    "           violate_b = violate\n",
    "\n",
    "        if time.monotonic() - start_time > 30.0:\n",
    "            start_time =  time.monotonic()\n",
    "            violate_a = violate\n",
    "            if len(list(violate_b & violate_a)) % 2== 0:\n",
    "                self.violate_t += list(violate_b & violate_a)\n",
    "                self.violate_t = list(set(self.violate_t))\n",
    "            violate_b = violate\n",
    "            \n",
    "        violate = list(violate)\n",
    "        \n",
    "        return violate, self.violate_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP2. 画像出力用のユーティリティ関数も少し修正"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create output image(frame)\n",
    "def create_output_image_with_violation(image, detected_person_list, violate, violate_t, mosaic=False):\n",
    "    for detected_person in detected_person_list:\n",
    "        id = detected_person.id\n",
    "        color = (0, 0, 0)\n",
    "        xmin, ymin, xmax, ymax = detected_person.pos\n",
    "        if mosaic:\n",
    "            image = mosaic_area(image, xmin, ymin, xmax, ymax)\n",
    "        if detected_person.id in violate:\n",
    "           color = (0, 255, 0)\n",
    "        if detected_person.id in violate_t:\n",
    "           color = (0, 0, 255)\n",
    "        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), color, 5)\n",
    "    font = cv2.FONT_HERSHEY_COMPLEX\n",
    "    cv2.putText(image, str(len(violate_t)), (50, 150), font, 4, (0, 0, 255), 2, cv2.LINE_AA)              \n",
    "    image = cv2.resize(image, dsize=(600, 360))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP3. メイン関数を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(video_file):\n",
    "    \n",
    "    #Instantiate two AI models\n",
    "    iecore = IECore()\n",
    "    detector = PersonDetector(iecore, \"intel/person-detection-retail-0013/FP32/person-detection-retail-0013\")\n",
    "    identifier = PersonIdentifier(iecore, \"intel/person-reidentification-retail-0287/FP32/person-reidentification-retail-0287\")\n",
    "    \n",
    "    violation_judger = SocialDistanceViolationJudge()\n",
    "   \n",
    "    #Please enter the name of the video file you want to process\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    \n",
    "    start_time = time.monotonic()\n",
    "    \n",
    "    index = 0 #Index setting\n",
    "    while cv2.waitKey(1) != 27:    \n",
    "        index += 1\n",
    "        \n",
    "        frame = cap.read()\n",
    "        if frame[0] == False:\n",
    "            return\n",
    "        \n",
    "        image = frame[1]\n",
    "\n",
    "        #AI model \"human recognition　settings\" inference\n",
    "        detected_person_list = detector.inference_and_get_person_bbox(image)\n",
    "\n",
    "        for detected_person in detected_person_list:\n",
    "            xmin, ymin, xmax, ymax = detected_person.pos\n",
    "            person_image = image[ymin:ymax, xmin:xmax] \n",
    "            feature = identifier.inference_and_get_feature(person_image)    \n",
    "            detected_person.feature = feature\n",
    "\n",
    "        #Register vectors into the database\n",
    "        identifier.register_into_database(detected_person_list)\n",
    "       \n",
    "        #Judging a violation of Social Distance\n",
    "        violate, violate_t = violation_judger.judge_violation_of_social_distance(detected_person_list, index, start_time)\n",
    "        \n",
    "        #Window display processing           \n",
    "        image = create_output_image_with_violation(image, detected_person_list, violate, violate_t)\n",
    "        #cv2.imshow('Frame', image)\n",
    "        clear_output(wait=True)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        f = io.BytesIO()\n",
    "        PIL.Image.fromarray(image).save(f, 'jpeg')\n",
    "        IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
    "        \n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP4. 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main('people.264')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## おしまい！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
