{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ソーシャルディスタンスを検知するアプリを作ろう\n",
    "インテル® OpenVINO™ ツールキットの事前学習済みモデルを使ってソーシャルディスタンスを検知するアプリケーションを作成します。事前学習済みモデルを利用することで簡単にAI機能を含んだアプリケーションを開発できることをご体験ください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第1章 人物検出\n",
    "まずはじめに動画や画像から人物を物体検出するプログラムを作成します。人物を検出するモデルはOpenVINOの事前学習済みモデルを使用します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP0. 事前学習済みモデルのダウンロード\n",
    "今回は'person-detection-retail-0013'というモデルを使用するので、下記のコマンドでOpenVINO Model Zoo（OpenVINOのモデルレポジトリー）よりダウンロードしてきます。\n",
    "\n",
    "ちなみに、このツールはOpenVINOに付属しているModel Downloaderというツールです。通常事前学習済みモデルをダウンロードする際はこちらを使うのがデフォルトです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 $INTEL_OPENVINO_DIR/deployment_tools/tools/model_downloader/downloader.py --name person-detection-retail-0013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、下記コマンドを打つと、OpenVINO Model Zooに格納されているすべての事前学習済みモデルの一覧を表示します。お試しください。\n",
    "```Bash\n",
    "!python3 $INTEL_OPENVINO_DIR/deployment_tools/tools/model_downloader/downloader.py --print_all\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP1. モジュールのインポート\n",
    "とりあえずこの後使うもの全部をここでインポートしておきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import os\n",
    "import io\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "\n",
    "import logging as log\n",
    "from PIL import Image\n",
    "import PIL\n",
    "\n",
    "from munkres import Munkres\n",
    "\n",
    "from openvino.inference_engine import IENetwork, IECore\n",
    "\n",
    "import IPython.display\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP2. 必要なクラスを定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#検出したPersonの各種情報を保持しておくためのデータホルダークラス\n",
    "class DetectedPersonInfo:\n",
    "    def __init__(self, position, feature=[], id=-1):\n",
    "        self.pos = position\n",
    "        self.feature = feature\n",
    "        self.time = time.monotonic()\n",
    "        self.id = id\n",
    "\n",
    "#Personを検出するためのディープラーニング推論をOpenVINOで実行するためのクラス\n",
    "class PersonDetector:\n",
    "    \n",
    "    #検出した物体がPersonであるか否かを判定するための閾値\n",
    "    THRESHOLD = 0.3\n",
    "    \n",
    "    def __init__(self, iecore, model_path):\n",
    "        #AI model of human recognition　settings\n",
    "        self.net_h  = net = iecore.read_network(model = model_path+ \".xml\", weights = model_path + \".bin\") \n",
    "        self.input_name_h  = next(iter(self.net_h.inputs))                     \n",
    "        self.input_shape_h = self.net_h.inputs[self.input_name_h].shape           \n",
    "        self.out_name_h    = next(iter(self.net_h.outputs))                    \n",
    "        self.out_shape_h   = self.net_h.outputs[self.out_name_h].shape  \n",
    "        self.exec_net_h    = iecore.load_network(self.net_h, 'CPU')\n",
    "    \n",
    "    def inference_and_get_person_bbox(self, image):\n",
    "        #AI model \"human recognition　settings\" inference\n",
    "        in_frame = cv2.resize(image, (self.input_shape_h[3], self.input_shape_h[2]))\n",
    "        in_frame = in_frame.transpose((2, 0, 1))\n",
    "        in_frame = in_frame.reshape(self.input_shape_h)\n",
    "        res_h = self.exec_net_h.infer(inputs={self.input_name_h: in_frame})\n",
    "        person_bbox_list = res_h[self.out_name_h][0][0]\n",
    "        \n",
    "        detected_person_list = []\n",
    "        for person_bbox in person_bbox_list:\n",
    "            probability = person_bbox[2]\n",
    "            if probability > PersonDetector.THRESHOLD: \n",
    "                frame = image\n",
    "                xmin = abs(int(person_bbox[3] * frame.shape[1]))\n",
    "                ymin = abs(int(person_bbox[4] * frame.shape[0]))\n",
    "                xmax = abs(int(person_bbox[5] * frame.shape[1]))\n",
    "                ymax = abs(int(person_bbox[6] * frame.shape[0]))\n",
    "\n",
    "                person_image = frame[ymin:ymax, xmin:xmax]   \n",
    "                detected_person_list.append(DetectedPersonInfo([xmin, ymin, xmax, ymax]))\n",
    "        return detected_person_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP3. 各種ユーティリティー関数を定義\n",
    "- 検出した人物にモザイクをかける関数（プライバシー保護用。ハンズオンでは使いませんが。。。）\n",
    "- モデルの推論結果を反映した出力画像を生成する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#検出したPersonにモザイク処理をするための関数\n",
    "def mosaic_area(img, xmin, ymin, xmax, ymax, ratio=0.05):\n",
    "    dst = img.copy()\n",
    "    dst[ymin:ymax,xmin:xmax] = mosaic(dst[ymin:ymax,xmin:xmax], ratio)\n",
    "    return dst\n",
    "\n",
    "def mosaic(img, ratio=0.05):\n",
    "    small = cv2.resize(img, None, fx=ratio, fy=ratio, interpolation=cv2.INTER_NEAREST)\n",
    "    big = cv2.resize(small, img.shape[:2][::-1], interpolation=cv2.INTER_NEAREST)\n",
    "    return big\n",
    "\n",
    "\n",
    "#推論結果を反映した画面出力用の画像（フレーム）を作成する関数\n",
    "def create_output_image(image, detected_person_list, mosaic=False):\n",
    "    for detected_person in detected_person_list:\n",
    "        id = detected_person.id\n",
    "        color = (0, 0, 0)\n",
    "        xmin, ymin, xmax, ymax = detected_person.pos\n",
    "        if mosaic:\n",
    "            image = mosaic_area(image, xmin, ymin, xmax, ymax)\n",
    "        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), color, 5)\n",
    "    font = cv2.FONT_HERSHEY_COMPLEX\n",
    "    cv2.putText(image, 'PersonDetection', (100, 100), font, 1, (0, 0, 255), 2, cv2.LINE_AA)              \n",
    "    image = cv2.resize(image, dsize=(600, 360))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP4. メイン関数を定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(video_file):\n",
    "\n",
    "    #Instantiate two AI models\n",
    "    iecore = IECore()\n",
    "    detector = PersonDetector(iecore, \"intel/person-detection-retail-0013/FP32/person-detection-retail-0013\")\n",
    "   \n",
    "    #Please enter the name of the video file you want to process\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    while cv2.waitKey(1) != 27:        \n",
    "        #Read one frame from the video data\n",
    "        frame_data = cap.read()\n",
    "        if frame_data[0] == False:\n",
    "            return\n",
    "        \n",
    "        #Set captured frame\n",
    "        image = frame_data[1]\n",
    "\n",
    "        #Detect all person's bounding box in the captured frame\n",
    "        detected_person_list = detector.inference_and_get_person_bbox(image)\n",
    "\n",
    "        #Window display processing           \n",
    "        image = create_output_image(image, detected_person_list)\n",
    "        #cv2.imshow('Frame', image)\n",
    "        clear_output(wait=True)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        f = io.BytesIO()\n",
    "        PIL.Image.fromarray(image).save(f, 'jpeg')\n",
    "        IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
    "        \n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP5. 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main('town.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第2章 人物認証\n",
    "次に検出した人物を認証するプログラムを追加します。ひとつ前の人物検出は、あくまでもフレームごとに写っている人物たちをスナップショットで検出して、その結果を同じくフレームごとに出力しているだけです。つまり当該フレームの前後フレームとの依存性がないため、現在のフレームに映っている人物が、次のフレームに映っている人物と同一であることを紐づけることができません。\n",
    "\n",
    "この人物認証では、その問題を解決するためにフレーム毎に人物の認証を行い、複数フレーム間で同一人物を特定することができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP0. 事前学習済みモデルのダウンロード\n",
    "OpenVINOの事前学習済みモデル \"person-reidentification-retail-0287\"をダウンロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 $INTEL_OPENVINO_DIR/deployment_tools/tools/model_downloader/downloader.py --name person-reidentification-retail-0287"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP1. 人物認証処理を実行する用のクラスを定義\n",
    "このモデルが行うことは大きく2つです。\n",
    "- 前段の人物検出で検出された人物ごとに特徴量を抽出\n",
    "- その特徴量データをデータベース上で管理し、同一人物か否かを判断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonIdentifier:\n",
    "    def __init__(self, iecore, model_path):\n",
    "        #AI model of personal identification settings\n",
    "        self.net_p = iecore.read_network(model = model_path + \".xml\", weights = model_path + \".bin\") \n",
    "        self.input_name_p  = next(iter(self.net_p.inputs))                  \n",
    "        self.input_shape_p = self.net_p.inputs[self.input_name_p].shape        \n",
    "        self.out_name_p    = next(iter(self.net_p.outputs))                 \n",
    "        self.out_shape_p   = self.net_p.outputs[self.out_name_p].shape \n",
    "        self.exec_net_p    = iecore.load_network(self.net_p, 'CPU')\n",
    "        \n",
    "        #Person identification management\n",
    "        self.id_num = 0\n",
    "        self.dist_threshold = 1.0\n",
    "        self.timeout_threshold = 10000\n",
    "        self.feature_db = []\n",
    "    \n",
    "    def inference_and_get_feature(self, obj_img):\n",
    "        obj_img = cv2.resize(obj_img, (128, 256)) \n",
    "        obj_img = obj_img.transpose((2,0,1))\n",
    "        obj_img = np.expand_dims(obj_img, axis=0)\n",
    "\n",
    "        #AI model \"Personal recognition\" inference             \n",
    "        res_reid = self.exec_net_p.infer(inputs={ self.input_name_p : obj_img}) \n",
    "        feature = np.array(res_reid[self.out_name_p]).reshape((256))\n",
    "        return feature\n",
    "    \n",
    "    def register_into_database(self, detected_person_list):\n",
    "        hangarian = Munkres()\n",
    "        dist_matrix = [ [ distance.cosine(obj_db.feature, obj_cam.feature) for obj_db in self.feature_db ] for obj_cam in detected_person_list ]\n",
    "        combination = hangarian.compute(dist_matrix)      \n",
    "        for idx_obj, idx_db in combination:\n",
    "            if detected_person_list[idx_obj].id!=-1: \n",
    "                continue \n",
    "            dist = distance.cosine(detected_person_list[idx_obj].feature, self.feature_db[idx_db].feature)\n",
    "            if dist < self.dist_threshold:\n",
    "                self.feature_db[idx_db].time = time.monotonic()            \n",
    "                detected_person_list[idx_obj].id = self.feature_db[idx_db].id              \n",
    "        del hangarian\n",
    "        \n",
    "        for detected_person in detected_person_list:\n",
    "            if detected_person.id == -1:\n",
    "                xmin, ymin, xmax, ymax = detected_person.pos\n",
    "                detected_person.id = self.id_num\n",
    "                self.feature_db.append(detected_person)\n",
    "                self.id_num += 1\n",
    "        \n",
    "        for i, db in enumerate(self.feature_db):\n",
    "            if time.monotonic() - db.time > self.timeout_threshold:\n",
    "                self.feature_db.pop(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP2. 画面表示用の関数を少し修正\n",
    "人物認証の結果が分かりやすいように画面表示時に検出した人物のBounding BoxにIDを表示するようにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_output_image_with_id(image, detected_person_list, mosaic=False):\n",
    "    for detected_person in detected_person_list:\n",
    "        id = detected_person.id\n",
    "        color = (0, 0, 0)\n",
    "        xmin, ymin, xmax, ymax = detected_person.pos\n",
    "        if mosaic:\n",
    "            image = mosaic_area(image, xmin, ymin, xmax, ymax)\n",
    "        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), color, 5)\n",
    "        cv2.putText(image, str(id), (xmin, ymin - 7), cv2.FONT_HERSHEY_COMPLEX, 1.0, color, 1)\n",
    "    font = cv2.FONT_HERSHEY_COMPLEX\n",
    "    cv2.putText(image, 'PersonIdentification', (100, 100), font, 1, (0, 0, 255), 2, cv2.LINE_AA)              \n",
    "    image = cv2.resize(image, dsize=(600, 360))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP3. メイン関数を修正\n",
    "人物認証用の処理を追加します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(video_file):\n",
    "    #Instantiate two AI models\n",
    "    iecore = IECore()\n",
    "    detector = PersonDetector(iecore, \"intel/person-detection-retail-0013/FP32/person-detection-retail-0013\")\n",
    "    identifier = PersonIdentifier(iecore, \"intel/person-reidentification-retail-0287/FP32/person-reidentification-retail-0287\")\n",
    "       \n",
    "    #Please enter the name of the video file you want to process\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    while cv2.waitKey(1) != 27:        \n",
    "        frame = cap.read()\n",
    "        if frame[0] == False:\n",
    "            return\n",
    "        \n",
    "        image = frame[1]\n",
    "\n",
    "        #AI model \"human recognition　settings\" inference\n",
    "        detected_person_list = detector.inference_and_get_person_bbox(image)\n",
    "\n",
    "        for detected_person in detected_person_list:\n",
    "            xmin, ymin, xmax, ymax = detected_person.pos\n",
    "            person_image = image[ymin:ymax, xmin:xmax] \n",
    "            feature = identifier.inference_and_get_feature(person_image)    \n",
    "            detected_person.feature = feature\n",
    "\n",
    "        #Register vectors into the database\n",
    "        identifier.register_into_database(detected_person_list)\n",
    "        \n",
    "        #Window display processing           \n",
    "        image = create_output_image_with_id(image, detected_person_list)\n",
    "        #cv2.imshow('Frame', image)\n",
    "        clear_output(wait=True)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        f = io.BytesIO()\n",
    "        PIL.Image.fromarray(image).save(f, 'jpeg')\n",
    "        IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
    "        \n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP4. 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main('town.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第3章 ソーシャルディスタンス検知\n",
    "これまで人物を検出して、それらの人物を認証し、フレーム間で関連性を持たせるようにプログラムを改良してきました。ようやくソーシャルディスタンスを検出する処理を追加できます。\n",
    "\n",
    "といっても、この処理にはAIは用いません。つまり、モデルを使いません。モデルが出力した情報をもとに独自ロジックでもってソーシャルディスタンスを検出し、一定時間近い距離にいる人たちに対してアラートを表示します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP1. ソーシャルディスタンス検知処理用のクラスを定義\n",
    "前述の通り、AIは使いません。人物検出や人物認証の結果を活用して計算していきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SocialDistanceViolationJudge:\n",
    "    '''\n",
    "    Please custom Social_parameter\n",
    "    SOCIAL_PARAMETER determines Social_Distance.\n",
    "    But that's not the actual distance. This is the distance between the x and y coordinates on the screen.\n",
    "    '''\n",
    "    SOCIAL_PARAMETER = 50.0\n",
    "    \n",
    "    # Threshold to decide whether it violates longer time.\n",
    "    TIME_THRESHOLD = 30.0\n",
    "\n",
    "    def __init__(self):\n",
    "        self.violate_person_id_list = list()\n",
    "        self.start_time = time.monotonic()\n",
    "        self.first_time = True\n",
    "   \n",
    "    def judge_violation_of_social_distance(self, detected_person_list):\n",
    "        warning_person_id_set = set() \n",
    "        old_warning_person_id_set = set()\n",
    "        \n",
    "        centroids = np.array([[(detected_person.pos[2]-detected_person.pos[0])/2+detected_person.pos[0],\n",
    "                               (detected_person.pos[3]-detected_person.pos[1])/2+detected_person.pos[1]] \n",
    "                              for detected_person in detected_person_list])\n",
    "        D_1 = distance.cdist(centroids, centroids, metric = \"euclidean\")        \n",
    "        for i in range(0, D_1.shape[0]):\n",
    "            for j in range(i+1, D_1.shape[1]):\n",
    "                if  (D_1[i,j] != 0.0 and D_1[i, j] < SocialDistanceViolationJudge.SOCIAL_PARAMETER):\n",
    "                    warning_person_id_set.add(detected_person_list[i].id)\n",
    "                    warning_person_id_set.add(detected_person_list[j].id)                 \n",
    "        if self.first_time:\n",
    "            old_warning_person_id_set = warning_person_id_set\n",
    "            self.first_time = False\n",
    "\n",
    "        if (time.monotonic() - self.start_time) > SocialDistanceViolationJudge.TIME_THRESHOLD:\n",
    "            self.start_time =  time.monotonic()\n",
    "            latest_warning_person_id_set = warning_person_id_set\n",
    "            if len(list(old_warning_person_id_set & latest_warning_person_id_set)) % 2 == 0:\n",
    "                self.violate_person_id_list += list(old_warning_person_id_set & latest_warning_person_id_set)\n",
    "                self.violate_person_id_list = list(set(self.violate_person_id_list))\n",
    "            old_warning_person_id_set = warning_person_id_set\n",
    "            \n",
    "        warning_person_id_list = list(warning_person_id_set)\n",
    "        \n",
    "        return warning_person_id_list, self.violate_person_id_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP2. 画像出力用のユーティリティ関数も少し修正\n",
    "今回、近い距離に一瞬でもいる人たちのBounding Boxを黄色に、近い距離に一定時間以上いる人たちのBounding Boxを赤色にして表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create output image(frame)\n",
    "def create_output_image_with_violation(image, detected_person_list, warning_id_list, violate_id_list, mosaic=False):\n",
    "    for detected_person in detected_person_list:\n",
    "        id = detected_person.id\n",
    "        color = (0, 0, 0)\n",
    "        xmin, ymin, xmax, ymax = detected_person.pos\n",
    "        if mosaic:\n",
    "            image = mosaic_area(image, xmin, ymin, xmax, ymax)\n",
    "        if detected_person.id in warning_id_list:\n",
    "           color = (0, 255, 0)\n",
    "        if detected_person.id in violate_id_list:\n",
    "           color = (0, 0, 255)\n",
    "        cv2.rectangle(image, (xmin, ymin), (xmax, ymax), color, 5)\n",
    "    font = cv2.FONT_HERSHEY_COMPLEX\n",
    "    cv2.putText(image, str(len(violate_id_list)), (100, 100), font, 1, (0, 0, 255), 2, cv2.LINE_AA)              \n",
    "    image = cv2.resize(image, dsize=(600, 360))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP3. メイン関数を修正\n",
    "ソーシャルディスタンス検知系の処理を追加します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(video_file):\n",
    "    \n",
    "    #Instantiate two AI models\n",
    "    iecore = IECore()\n",
    "    detector = PersonDetector(iecore, \"intel/person-detection-retail-0013/FP32/person-detection-retail-0013\")\n",
    "    identifier = PersonIdentifier(iecore, \"intel/person-reidentification-retail-0287/FP32/person-reidentification-retail-0287\")\n",
    "    \n",
    "    # ソーシャルディスタンスが近い人たちを検出するオブジェクト\n",
    "    violation_judger = SocialDistanceViolationJudge()\n",
    "   \n",
    "    #Please enter the name of the video file you want to process\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    while cv2.waitKey(1) != 27:    \n",
    "        frame = cap.read()\n",
    "        if frame[0] == False:\n",
    "            return\n",
    "        \n",
    "        image = frame[1]\n",
    "\n",
    "        #AI model \"human recognition　settings\" inference\n",
    "        detected_person_list = detector.inference_and_get_person_bbox(image)\n",
    "\n",
    "        for detected_person in detected_person_list:\n",
    "            xmin, ymin, xmax, ymax = detected_person.pos\n",
    "            person_image = image[ymin:ymax, xmin:xmax] \n",
    "            feature = identifier.inference_and_get_feature(person_image)    \n",
    "            detected_person.feature = feature\n",
    "\n",
    "        #Register vectors into the database\n",
    "        identifier.register_into_database(detected_person_list)\n",
    "       \n",
    "        #Judging a violation of Social Distance\n",
    "        warning_id_list, violate_id_list = violation_judger.judge_violation_of_social_distance(detected_person_list)\n",
    "        \n",
    "        #Window display processing           \n",
    "        image = create_output_image_with_violation(image, detected_person_list, warning_id_list, violate_id_list)\n",
    "        #cv2.imshow('Frame', image)\n",
    "        clear_output(wait=True)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        f = io.BytesIO()\n",
    "        PIL.Image.fromarray(image).save(f, 'jpeg')\n",
    "        IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
    "        \n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP4. 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main('town.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "おつかれさまでした。ここまででアプリケーションの開発は一旦完了です。\n",
    "\n",
    "一度[READ.ME](https://github.com/hiouchiy/intel_ai_openvino_hands_on)へお戻りいただき、続く応用編を実行するための準備を行ってください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【応用編】OpenVINO Model Serverを使ってモデルをWeb API化する\n",
    "OpenVINO Model ServerはOpenVINOのモデル（IR形式に変換されたモデル）をWeb APIとしてデプロイするためのWebフレームワークです。似たようなツールにTensorFlow版のTF Servingがありますが、まさにTF ServingのOpenVINO版といっても過言ではなく、APIなど非常に似た設計になっております。したがって、TF Servingベースのコードを非常に少ない工数（場合によっては工数なし）でOpenVINO Model Serverで使用できる形に移行可能です。\n",
    "\n",
    "OpenVINO Model ServerはDockerイメージとして提供されているので、基本的にはDockerがインストールされていることが前提です。[READ.ME](https://github.com/hiouchiy/intel_ai_openvino_hands_on)にイメージお取得方法及び起動方法が記載されておりますのでまずはそちらを実行したうえで以降の作業を行ってください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP0. 追加モジュールをインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-serving-api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP1. モジュールのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import datetime\n",
    "import grpc\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow import make_tensor_proto, make_ndarray\n",
    "from tensorflow_serving.apis import predict_pb2\n",
    "from tensorflow_serving.apis import prediction_service_pb2_grpc, get_model_metadata_pb2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP2. 人物検出用モデルを実行するクラスをModel Serverへ問い合わせるように作成する\n",
    "今回は人物検出モデルのみWeb API化しています。なので、PersonDetectorクラスを継承したRemotePersonDetectorクラスを作成し、そこにModel Serverにアクセスするための処理を記述します。ちなみに、Model ServerがサポートしているプロトコルはgRPCおよびHTTPです。デフォルトがgRPCであるため、このプログラムもgRPCのクライアントとして記述してあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Personを検出するためのディープラーニング推論をOpenVINO Model Sererへ問い合わせて実行するためのクラス\n",
    "class RemotePersonDetector(PersonDetector):\n",
    "    \n",
    "    #検出した物体がPersonであるか否かを判定するための閾値\n",
    "    THRESHOLD = 0.3\n",
    "    \n",
    "    def __init__(self, grpc_address='localhost', grpc_port=9000, model_name='person-detection', model_version=None):\n",
    "        #Settings for accessing model server\n",
    "        self.grpc_address = grpc_address\n",
    "        self.grpc_port = grpc_port\n",
    "        self.model_name = model_name\n",
    "        self.model_version = model_version\n",
    "        channel = grpc.insecure_channel(\"{}:{}\".format(self.grpc_address, self.grpc_port))\n",
    "        self.stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "        \n",
    "        # Get input shape info from Model Server\n",
    "        self.input_name, input_shape, self.output_name, output_shape = self.__get_input_name_and_shape__()\n",
    "        self.input_height = input_shape[2]\n",
    "        self.input_width = input_shape[3]\n",
    "    \n",
    "    def __get_input_name_and_shape__(self):\n",
    "        metadata_field = \"signature_def\"\n",
    "        request = get_model_metadata_pb2.GetModelMetadataRequest()\n",
    "        request.model_spec.name = self.model_name\n",
    "        if self.model_version is not None:\n",
    "            request.model_spec.version.value = self.model_version\n",
    "        request.metadata_field.append(metadata_field)\n",
    "\n",
    "        result = self.stub.GetModelMetadata(request, 10.0) # result includes a dictionary with all model outputs\n",
    "        input_metadata, output_metadata = self.__get_input_and_output_meta_data__(result)\n",
    "        input_blob = next(iter(input_metadata.keys()))\n",
    "        output_blob = next(iter(output_metadata.keys()))\n",
    "        return input_blob, input_metadata[input_blob]['shape'], output_blob, output_metadata[output_blob]['shape']\n",
    "    \n",
    "    def __get_input_and_output_meta_data__(self, response):\n",
    "        signature_def = response.metadata['signature_def']\n",
    "        signature_map = get_model_metadata_pb2.SignatureDefMap()\n",
    "        signature_map.ParseFromString(signature_def.value)\n",
    "        serving_default = signature_map.ListFields()[0][1]['serving_default']\n",
    "        serving_inputs = serving_default.inputs\n",
    "        input_blobs_keys = {key: {} for key in serving_inputs.keys()}\n",
    "        tensor_shape = {key: serving_inputs[key].tensor_shape\n",
    "                        for key in serving_inputs.keys()}\n",
    "        for input_blob in input_blobs_keys:\n",
    "            inputs_shape = [d.size for d in tensor_shape[input_blob].dim]\n",
    "            tensor_dtype = serving_inputs[input_blob].dtype\n",
    "            input_blobs_keys[input_blob].update({'shape': inputs_shape})\n",
    "            input_blobs_keys[input_blob].update({'dtype': tensor_dtype})\n",
    "        \n",
    "        serving_outputs = serving_default.outputs\n",
    "        output_blobs_keys = {key: {} for key in serving_outputs.keys()}\n",
    "        tensor_shape = {key: serving_outputs[key].tensor_shape\n",
    "                        for key in serving_outputs.keys()}\n",
    "        for output_blob in output_blobs_keys:\n",
    "            outputs_shape = [d.size for d in tensor_shape[output_blob].dim]\n",
    "            tensor_dtype = serving_outputs[output_blob].dtype\n",
    "            output_blobs_keys[output_blob].update({'shape': outputs_shape})\n",
    "            output_blobs_keys[output_blob].update({'dtype': tensor_dtype})\n",
    "\n",
    "        return input_blobs_keys, output_blobs_keys\n",
    "    \n",
    "    def inference_and_get_person_bbox(self, image):\n",
    "        # 画像データに対する前処理\n",
    "        img = cv2.resize(image, (self.input_width, self.input_height))\n",
    "        img = img.transpose((2, 0, 1))\n",
    "        img = img.reshape(1, 3, self.input_height, self.input_width)\n",
    "        img = img.astype(np.float32)\n",
    "        \n",
    "        # Model ServerにgRPCでアクセスしてモデルをコール\n",
    "        request = predict_pb2.PredictRequest()\n",
    "        request.model_spec.name = self.model_name\n",
    "        request.inputs[self.input_name].CopyFrom(make_tensor_proto(img, shape=(img.shape)))\n",
    "        result = self.stub.Predict(request, 10.0) # result includes a dictionary with all model outputs\n",
    "        res_h = make_ndarray(result.outputs[self.output_name])\n",
    "        person_bbox_list = res_h[0][0]\n",
    "        \n",
    "        # モデルの出力から必要なデータを抽出し、呼び出し元へ返す\n",
    "        detected_person_list = []\n",
    "        for person_bbox in person_bbox_list:\n",
    "            probability = person_bbox[2]\n",
    "            if probability > PersonDetector.THRESHOLD: \n",
    "                frame = image\n",
    "                xmin = abs(int(person_bbox[3] * frame.shape[1]))\n",
    "                ymin = abs(int(person_bbox[4] * frame.shape[0]))\n",
    "                xmax = abs(int(person_bbox[5] * frame.shape[1]))\n",
    "                ymax = abs(int(person_bbox[6] * frame.shape[0]))\n",
    "\n",
    "                person_image = frame[ymin:ymax, xmin:xmax]   \n",
    "                detected_person_list.append(DetectedPersonInfo([xmin, ymin, xmax, ymax]))\n",
    "        \n",
    "        return detected_person_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP3. メイン関数を修正\n",
    "こちらはRemotePersonDetectorを初期化する以外、特段変更点はありません。\n",
    "なお、Model Serverは同一のホストOS上で稼働しているはずなので、宛先のアドレスはホストOSのIPアドレスを指定ください。ポート番号は9000を指定していますが、これはModel Serverのデフォルトです。model_nameはModel Server上で稼働しているモデルを識別するための名称です。Model Server起動時にしているものを入力ください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(video_file):\n",
    "    \n",
    "    #Instantiate two AI models\n",
    "    iecore = IECore()\n",
    "    # 修正したのこの一行だけ↓\n",
    "    detector = RemotePersonDetector(grpc_address='192.168.145.33', grpc_port='9000', model_name='person-detection')\n",
    "    identifier = PersonIdentifier(iecore, \"intel/person-reidentification-retail-0287/FP32/person-reidentification-retail-0287\")\n",
    "    \n",
    "    # ソーシャルディスタンスが近い人たちを検出するオブジェクト\n",
    "    violation_judger = SocialDistanceViolationJudge()\n",
    "   \n",
    "    #Please enter the name of the video file you want to process\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    while cv2.waitKey(1) != 27:    \n",
    "        frame = cap.read()\n",
    "        if frame[0] == False:\n",
    "            return\n",
    "        \n",
    "        image = frame[1]\n",
    "\n",
    "        #AI model \"human recognition　settings\" inference\n",
    "        detected_person_list = detector.inference_and_get_person_bbox(image)\n",
    "\n",
    "        for detected_person in detected_person_list:\n",
    "            xmin, ymin, xmax, ymax = detected_person.pos\n",
    "            person_image = image[ymin:ymax, xmin:xmax] \n",
    "            feature = identifier.inference_and_get_feature(person_image)    \n",
    "            detected_person.feature = feature\n",
    "\n",
    "        #Register vectors into the database\n",
    "        identifier.register_into_database(detected_person_list)\n",
    "       \n",
    "        #Judging a violation of Social Distance\n",
    "        warning_id_list, violate_id_list = violation_judger.judge_violation_of_social_distance(detected_person_list)\n",
    "        \n",
    "        #Window display processing\n",
    "        image = create_output_image_with_violation(image, detected_person_list, warning_id_list, violate_id_list)\n",
    "        #cv2.imshow('Frame', image)\n",
    "        clear_output(wait=True)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        f = io.BytesIO()\n",
    "        PIL.Image.fromarray(image).save(f, 'jpeg')\n",
    "        IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
    "        \n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実行\n",
    "実行結果は変わらないはずです。ただし、人物検出の推論処理のみModel Server上に送られて実行されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main('people.264')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このようにOpenVINO Model Serverを使うことで、既存のOpenVINOのモデル（事前学習済みモデル、カスタムモデル問わず）を簡単にWeb API化できます。ただし、基本的にはモデルの推論処理のみがAPI化されるので、マイクロサービスのような細かい粒度のサービスを作成する際などに非常に向いております。逆に、前処理／後処理などはクライアント側、または、間にもう1サーバー挟んで行うなどの工夫が必要です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# おしまい！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
